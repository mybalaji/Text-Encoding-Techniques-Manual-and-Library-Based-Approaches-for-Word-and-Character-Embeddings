{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPYUi7OeqAt2uSXtLckv6D5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"id":"_0wkPe4dCvJK","executionInfo":{"status":"ok","timestamp":1700442972818,"user_tz":-330,"elapsed":1824,"user":{"displayName":"Balaji Balla","userId":"10795341893185349647"}}},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import os"]},{"cell_type":"code","source":["import numpy as np\n","docs = \"Can I eat the Pizza\".lower().split()\n","doc1 = set(docs)\n","doc1 = sorted(doc1)\n","print (\"\\nvalues: \", doc1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KO3AEnW6DE3r","executionInfo":{"status":"ok","timestamp":1700442972818,"user_tz":-330,"elapsed":11,"user":{"displayName":"Balaji Balla","userId":"10795341893185349647"}},"outputId":"29669e16-964e-4966-fdff-3108397326a4"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","values:  ['can', 'eat', 'i', 'pizza', 'the']\n"]}]},{"cell_type":"code","source":["integer_encoded = []\n","for i in docs:\n","    v = np.where( np.array(doc1) == i)[0][0]\n","    integer_encoded.append(v)\n","print (\"\\ninteger encoded: \",integer_encoded)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AUZCX3N7DZbU","executionInfo":{"status":"ok","timestamp":1700442972819,"user_tz":-330,"elapsed":9,"user":{"displayName":"Balaji Balla","userId":"10795341893185349647"}},"outputId":"8137589d-c3ce-4761-c3a0-e4a6b529c298"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","integer encoded:  [0, 2, 1, 4, 3]\n"]}]},{"cell_type":"code","source":["def get_vec(len_doc,word):\n","    empty_vector = [0] * len_doc\n","    vect = 0\n","    find = np.where( np.array(doc1) == word)[0][0]\n","    empty_vector[find] = 1\n","    return empty_vector\n","\n","def get_matrix(doc1):\n","    mat = []\n","    len_doc = len(doc1)\n","    for i in docs:\n","        vec = get_vec(len_doc,i)\n","        mat.append(vec)\n","\n","    return np.asarray(mat)\n","\n","print (\"\\nMATRIX:\")\n","print (get_matrix(doc1))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wMhll65DDoB_","executionInfo":{"status":"ok","timestamp":1700442972819,"user_tz":-330,"elapsed":7,"user":{"displayName":"Balaji Balla","userId":"10795341893185349647"}},"outputId":"634f664f-eb3c-4931-bef0-033118799698"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","MATRIX:\n","[[1 0 0 0 0]\n"," [0 0 1 0 0]\n"," [0 1 0 0 0]\n"," [0 0 0 0 1]\n"," [0 0 0 1 0]]\n"]}]},{"cell_type":"code","source":["import numpy as np\n","\n","docs = \"Can I eat the Pizza\".lower()\n","doc1 = set(docs)\n","doc1 = sorted(doc1)\n","\n","print(\"\\nvalues: \", doc1)\n","\n","integer_encoded = []\n","for i in docs:\n","    v = np.where(np.array(doc1) == i)[0][0]\n","    integer_encoded.append(v)\n","\n","print(\"\\ninteger encoded: \", integer_encoded)\n","\n","\n","def get_vec(len_doc, char):\n","    empty_vector = [0] * len_doc\n","    find = np.where(np.array(doc1) == char)[0][0]\n","    empty_vector[find] = 1\n","    return empty_vector\n","\n","\n","def get_matrix(doc1):\n","    mat = []\n","    len_doc = len(doc1)\n","    for i in docs:\n","        vec = get_vec(len_doc, i)\n","        mat.append(vec)\n","\n","    return np.asarray(mat)\n","\n","print(\"\\nMATRIX:\")\n","print(get_matrix(doc1))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yn1fsa-0GFOB","executionInfo":{"status":"ok","timestamp":1700442974523,"user_tz":-330,"elapsed":1710,"user":{"displayName":"Balaji Balla","userId":"10795341893185349647"}},"outputId":"8e9d9081-014d-49e7-b7ec-fa4b78229432"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","values:  [' ', 'a', 'c', 'e', 'h', 'i', 'n', 'p', 't', 'z']\n","\n","integer encoded:  [2, 1, 6, 0, 5, 0, 3, 1, 8, 0, 8, 4, 3, 0, 7, 5, 9, 9, 1]\n","\n","MATRIX:\n","[[0 0 1 0 0 0 0 0 0 0]\n"," [0 1 0 0 0 0 0 0 0 0]\n"," [0 0 0 0 0 0 1 0 0 0]\n"," [1 0 0 0 0 0 0 0 0 0]\n"," [0 0 0 0 0 1 0 0 0 0]\n"," [1 0 0 0 0 0 0 0 0 0]\n"," [0 0 0 1 0 0 0 0 0 0]\n"," [0 1 0 0 0 0 0 0 0 0]\n"," [0 0 0 0 0 0 0 0 1 0]\n"," [1 0 0 0 0 0 0 0 0 0]\n"," [0 0 0 0 0 0 0 0 1 0]\n"," [0 0 0 0 1 0 0 0 0 0]\n"," [0 0 0 1 0 0 0 0 0 0]\n"," [1 0 0 0 0 0 0 0 0 0]\n"," [0 0 0 0 0 0 0 1 0 0]\n"," [0 0 0 0 0 1 0 0 0 0]\n"," [0 0 0 0 0 0 0 0 0 1]\n"," [0 0 0 0 0 0 0 0 0 1]\n"," [0 1 0 0 0 0 0 0 0 0]]\n"]}]},{"cell_type":"code","source":["import numpy as np\n","from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n","\n","# Sample list of words\n","words = ['apple', 'banana', 'orange']\n","\n","# Create a character vocabulary\n","chars = set(''.join(words))\n","char_vocab_size = len(chars)\n","\n","# Create a character-to-index mapping\n","char_to_index = {char: index for index, char in enumerate(chars)}\n","\n","# Encode characters using label encoding\n","encoded_words = [[char_to_index[char] for char in word] for word in words]\n","\n","# Convert encoded_words to a 2D array of strings\n","encoded_words = np.array([''.join(map(str, word)) for word in encoded_words]).reshape(-1, 1)\n","\n","# Perform one-hot encoding on character sequences\n","encoder = OneHotEncoder(categories='auto', sparse=False)\n","onehot_encoded_words = encoder.fit_transform(encoded_words)\n","\n","print(onehot_encoded_words)\n"],"metadata":{"id":"_7xsHZhQQKLP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1700443068401,"user_tz":-330,"elapsed":5,"user":{"displayName":"Balaji Balla","userId":"10795341893185349647"}},"outputId":"17831b3d-b30f-49be-fdc6-0dab9cae912b"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["[[0. 0. 1.]\n"," [1. 0. 0.]\n"," [0. 1. 0.]]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","\n","# Sample text data\n","texts = [\"one hot encoding example\", \"word embedding tutorial\", \"character one-hot\"]\n","\n","# Tokenize words\n","tokenizer_word = Tokenizer()\n","tokenizer_word.fit_on_texts(texts)\n","\n","# One-hot encode words\n","sequences_word = tokenizer_word.texts_to_sequences(texts)\n","one_hot_word = tokenizer_word.sequences_to_matrix(sequences_word, mode='binary')\n","\n","# Tokenize characters\n","tokenizer_char = Tokenizer(char_level=True)\n","tokenizer_char.fit_on_texts(texts)\n","\n","# One-hot encode characters\n","sequences_char = tokenizer_char.texts_to_sequences(texts)\n","one_hot_char = tokenizer_char.sequences_to_matrix(sequences_char, mode='binary')\n","\n","# Print results\n","print(\"One-hot encoding of words:\")\n","print(one_hot_word)\n","\n","print(\"\\nOne-hot encoding of characters:\")\n","print(one_hot_char)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v4vc9GFiTBxI","executionInfo":{"status":"ok","timestamp":1700443177760,"user_tz":-330,"elapsed":5967,"user":{"displayName":"Balaji Balla","userId":"10795341893185349647"}},"outputId":"fcf31745-b238-4626-c93d-24673ea04199"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["One-hot encoding of words:\n","[[0. 1. 1. 1. 1. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 1. 1. 1. 0.]\n"," [0. 1. 1. 0. 0. 0. 0. 0. 1.]]\n","\n","One-hot encoding of characters:\n","[[0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0.]\n"," [0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0.]\n"," [0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"BOlN9BBFT6eI"},"execution_count":null,"outputs":[]}]}